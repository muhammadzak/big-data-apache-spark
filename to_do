
Spark_terminalogy 

Cluster Manager
Is used to run the Spark Application in Cluster Mode
Application
User program built on Spark. Consists of,
Driver Program
The Program that has SparkContext. Acts as a coordinator for the Application
Executors
Runs computation & Stores Application Data
Are launched at the beginning of an Application & runs for the entire life time of an Application
Each Application gets it own Executors
An Application can have multiple Executors
An Executor is not shared by Multiple Applications
Provides in-memory storage for RDDs
For an Application, No >1 Executors run in the same Node
Task
Represents a unit of work in Spark
Gets executed in Executor
Job 
Parallel Computation consisting of multiple Tasks that gets spawned in response to Spark action


https://github.com/cloudera/hue/tree/master/apps/beeswax/data


https://www.snowflake.net/snowflake-and-spark-part-1-why-spark/


https://aws.amazon.com/blogs/big-data/using-spark-sql-for-etl/

Using Spark SQL for ETL



CREATE EXTERNAL TABLE IF NOT EXISTS UserMovieRatings (
userId int,
movieId int,
rating int,
unixTimestamp bigint
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY 't'
STORED AS TEXTFILE
LOCATION 's3://us-east-1.elasticmapreduce.samples/sparksql/movielens/user-movie-ratings'




CREATE EXTERNAL TABLE IF NOT EXISTS MovieDetails (
movieId int,
title string,
genres array<string>
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '#'
collection items terminated by '|'
STORED AS TEXTFILE
LOCATION 's3://us-east-1.elasticmapreduce.samples/sparksql/movielens/movie-details'


Transform the data using SparkSQL/Zeppelin


%sql SELECT title, count(*) numberOf5Ratings FROM usermovieratings r
JOIN moviedetails d ON (r.movieid = d.movieid)
WHERE rating = 5
GROUP BY title
ORDER BY numberOf5Ratings desc limit 5


%sql CREATE EXTERNAL TABLE IF NOT EXISTS UserDetails (
userId int,
age int,
gender CHAR(1),
occupation string,
zipCode String
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
LOCATION 's3://us-east-1.elasticmapreduce.samples/sparksql/movielens/user-details'

http://gethue.com/how-to-schedule-spark-jobs-with-spark-on-yarn-and-oozie/

https://aws.amazon.com/blogs/big-data/using-spark-sql-for-etl/

https://aws.amazon.com/blogs/big-data/using-spark-sql-for-etl/

https://aws.amazon.com/blogs/big-data/querying-amazon-kinesis-streams-directly-with-sql-and-spark-streaming/


Tuning Spark
1)Broadcasting 
Add my example of RDD

Data frame example

case class Employee(name:String, age:Int, depId: String)
case class Department(id: String, name: String)
 
val employeesRDD = sc.parallelize(Seq( 
    Employee("Mary", 33, "IT"), 
    Employee("Paul", 45, "IT"), 
    Employee("Peter", 26, "MKT"), 
    Employee("Jon", 34, "MKT"), 
    Employee("Sarah", 29, "IT"),
    Employee("Steve", 21, "Intern")
))
val departmentsRDD = sc.parallelize(Seq( 
    Department("IT", "IT  Department"),
    Department("MKT", "Marketing Department"),
    Department("FIN", "Finance & Controlling")
))
 
val employeesDF = employeesRDD.toDF
val departmentsDF = departmentsRDD.toDF


// materializing the department data
val tmpDepartments = broadcast(departmentsDF.as("departments"))
 
employeesDF.join(broadcast(tmpDepartments), 
   $"depId" === $"id",  // join by employees.depID == departments.id 
   "inner").show()


 http://henning.kropponline.de/2016/12/11/broadcast-join-with-spark/


2)serializer

  sparkSession = SparkSession.builder()
     .master("local")
     .appName("Word Count")
     .config.("spark.serializer", "org.apache.spark.serializer.KryoSerializer"
     .getOrCreate()


 https://ogirardot.wordpress.com/2015/01/09/changing-sparks-default-java-serialization-to-kryo/


3)Level of Parallelism

Clusters will not be fully utilized unless you set the level of parallelism for each operation high enough. Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument , or set the config property spark.default.parallelism to change the default. In general, its recommend 2-3 tasks per CPU core in a cluster.

4)Memory Usage of Reduce Tasks

Sometimes, you will get an OutOfMemoryError not because your RDDs don’t fit in memory, but because the working set of one of your tasks, such as one of the reduce tasks in groupByKey, was too large. Spark’s shuffle operations (sortByKey, groupByKey, reduceByKey, join, etc) build a hash table within each task to perform the grouping, which can often be large. The simplest fix here is to increase the level of parallelism, so that each task’s input set is smaller. 

5)Determining Memory Consumption to understand where to cahce?

The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it into cache, and look at the “Storage” page in the web UI. The page will tell you how much memory the RDD is occupying.

To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method This is useful for experimenting with different data layouts to trim memory usage, as well as determining the amount of space a broadcast variable will occupy on each executor heap.


https://spark.apache.org/docs/latest/tuning.html


http://hadooptutorial.info/bucketing-in-hive/

https://aws.amazon.com/blogs/big-data/anomaly-detection-using-pyspark-hive-and-hue-on-amazon-emr/

https://aws.amazon.com/blogs/big-data/tag/hive/

https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html


Project

https://www.youtube.com/watch?v=gsR1ljgZLq0&list=PL-x35fyliRwhEhG8K1ilX8v5qzRlfMJAV&index=5

can we have spark-multiple-contexts?


Although configuration option spark.driver.allowMultipleContexts exists, it is misleading because usage of multiple Spark contexts is discouraged. This option is used only for Spark internal tests and is not supposed to be used in user programs. You can get unexpected results while running more than one Spark context in a single JVM.

